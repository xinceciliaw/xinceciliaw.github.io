---
title: "Fourth Blog Post"
author: "Xin Wang"
date: "2022-07-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, eval=FALSE}
rmarkdown::render(
    '/Users/ceciliawang/Desktop/ST558/git/xinceciliaw.github.io/_Rmd/2022-07-19-fourth-blog-post.Rmd', 
    output_format = "github_document",
    output_dir = "/Users/ceciliawang/Desktop/ST558/git/xinceciliaw.github.io/_posts",
    output_options = list(
      html_preview = FALSE
    )
)
```


## Method I find most interesting

I think the method I find the most interesting is the KNN algorithm because it is very simple and efficient. The model representation of KNN is the entire training data set. The trick for this algorithm is how to determine the similarity between data instances. If your attributes have the same scale, the easiest technique is to use Euclidean distance, which you can calculate directly based on the difference between each input variable. Some advantages for KNN is it can be used for classification as well as for regression and new data can be added directly to the data set without having to be re-trained.

## Fit the Model 

```{r,message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
```

```{r}
set.seed(1)
trainIndex <- createDataPartition(iris$Species, p =0.6, list = FALSE)
Train<- iris[trainIndex,]
Test<- iris[-trainIndex,]
```

```{r}
fitknn <- train(Species ~ . ,
                data = Train,
                method = "knn",
                trControl = trainControl(method = 'cv', 
                                         number = 10),
                preProcess = c("center", "scale"),
                tuneGrid = data.frame(k = 1:15))
fitknn
```

```{r}
confusionMatrix(data = Test$Species, reference = predict(fitknn, newdata = Test))
```

We can see that the accuracy is 88.3% suggesting we may have an reliably accurate model.

